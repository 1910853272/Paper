# 三维重建

![iShot_2025-04-21_11.32.54](https://raw.githubusercontent.com/1910853272/image/master/img/202504211133096.png)

![iShot_2025-04-21_11.08.21](https://raw.githubusercontent.com/1910853272/image/master/img/202504211108940.png)

三维重建根据所用传感器的不同，可以分为**主动式三维重建**和**被动式三维重建**。

主动式三维重建根据传感器去主动探测深度信息，常用的传感器包括激光雷达，结构光和ToF等。

被动式三维重建通常只需要相机，适用场景较为广泛，根据算法输入视图数目的不同，可以分为单目深度估计、双目立体匹配和多视图三维重建三种方式。

![iShot_2025-04-20_20.37.23](https://raw.githubusercontent.com/1910853272/image/master/img/202504202145443.png)

+ 激光雷达  【准确/稀疏/场景受限/探测距离受限/贵】 --> 自动驾驶
+ 结构光/ToF 【快速出深度图/场景受限/探测距离受限/较贵】 --> KinectV1/V2、奥比中光、RealSense
+ 基于图像的三维重建 【成本低/算法性能在不断提升】
  + 单目深度估计
  + 双目立体匹配
  + 多视图三维重建

单目深度估计的深度图如果没有施加多视图的几何一致性（连续性）约束的话，那么重建三维几何的质量无法保证。

双目立体匹配计算的深度和双目相机的焦距和基线有关，如果需要获得较大的深度感知范围，则需要很大的基线距离，因此限制了双目立体匹配的应用范围。

多视图立体匹配的输入图像为多幅单目图像，图像无需进行校正， 采集成本低，通过多视图之间的相似性搜索进行深度图的预测。

![342](https://raw.githubusercontent.com/1910853272/image/master/img/202504211210369.png)

## 运动恢复结构SfM

运动恢复结构为输入图像进行相机参数估计和场景稀疏重建。标定照相机、计算照相机的内参数 K，通过求取本质矩阵以求取照相机外参数，并推断物点的三维坐标 X 进行欧式重建。

![](https://raw.githubusercontent.com/1910853272/image/master/img/202504202145399.png)

主要流程为：

1.使用尺度不变特征转换SIFT特征检测器提取特征点并计算特征点对应的描述子（descriptor），

2.使用KNN方法进行匹配，低于某个匹配数阈值的匹配对将会被移除。

2.对于保留下来的匹配对，使用RANSAC和八点法来估计基本矩阵，在估计基本矩阵时被判定为外点的匹配被看作是错误的匹配而被移除。



<img src="https://raw.githubusercontent.com/1910853272/image/master/img/202504202145231.png" alt="1" style="zoom: 25%;" />

## 尺度不变特征转换SIFT算法

<img src="https://raw.githubusercontent.com/1910853272/image/master/img/202504212042923.png" alt="1" style="zoom:50%;" />

**在不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向**。SIFT所查找到的关键点不会因光照，仿射变换和噪音等因素而变化如角点、边缘点、暗区的亮点及亮区的暗点等。

目标：寻找图像中具有旋转，平移，以及尺度不变性的那些特征点，并且最终用一个向量表示出来。

算法流程：

1.尺度空间极值检测： 原始图片先用高斯滤波做模糊操作的尺度变换，通过改变高斯核的标准差，得到S张图片后做成高斯金字塔；对于高斯金字塔的每一个level的S张图片， 相邻图片进行差分运算，得到S-1张高斯差分图像。

![](https://raw.githubusercontent.com/1910853272/image/master/img/202504211123404.png)

2.关键点定位：以上方法检测到的极值点是离散空间的极值点，之后通过拟合三维二次函数来精确确定关键点的位置和尺度，同时去除低对比度的关键点和不稳定的边缘响应点。

![4](https://raw.githubusercontent.com/1910853272/image/master/img/202504211127837.png)

3.方向确定：对于一个关键点， 要统计它邻域内各个点的梯度方向直方图，最高的bin对应的方向即定义为该关键点的主方向。

![5](https://raw.githubusercontent.com/1910853272/image/master/img/202504211131736.png)

4.关键点描述：得到特征点二维位置、尺度位置、主方向的具体信息后，最后要生成关键点信息的描述子，即用一个向量描述图像中的特征点信息。

将坐标轴旋转为特征点的主方向，将特征点周围的16 × 16 邻域分为4个8 × 8的区域，再划为2 × 2 区域，最后每个小区域为4 × 4的范围。统计每个4 × 4 区域的梯度方向直方图（直方图为8个bin，8个方向），故共计4 ∗ 4 ∗ 8 = 128 个bin。对应生成128维向量（值为梯度方向的幅值）即该点的特征描述子。

![56](https://raw.githubusercontent.com/1910853272/image/master/img/202504211143615.png)

之后再进行特征匹配，比较两张图片中哪些关键点比较相似。

## Harris角点检测算法

角点：图像里面的物体的角。在保留图像重要特征的同时，有效减少信息数据量。

每一个点为中心，取一个窗口，如果在各个方向上移动这个特征的小窗口，窗口内区域灰度都发生较大的变化，就认为这个窗口内有角点。

![12](https://raw.githubusercontent.com/1910853272/image/master/img/202504211102902.png)

算法流程：

1.当窗口同时向x和y两个方向移动时，计算窗口内部像素值变化量E(u,v)

2.对于每个窗口，计算对应的角点响应函数R

3.对于该函数阈值处理，如果R > threshold，表示该窗口对应一个角点特征

## 多视图立体视觉

**运动恢复结构（structure from motion，SfM）**技术，它通过一系列从不同位姿拍摄同一场景的普通 RGB 图像推断出拍摄各个图像时照相机的相对位姿和内外参数，从而构建出场景稀疏的三维结构。

**多视图立体视觉（multi-view stereo，MVS）**技术，它根据 SfM 得到的照相机相对位姿和内外参数估计出每张图像每个像素点的深度，从而重建出场景稠密的三维结构。

给定一对经过校准的、即经过 SfM 处理的双目立体视觉图像，**双视图立体视觉（two-view stereo）**的目标是计算出图像中每个像素的深度，从而得到场景稠密的深度图（depth map）

![34](https://raw.githubusercontent.com/1910853272/image/master/img/202504211206928.jpg)