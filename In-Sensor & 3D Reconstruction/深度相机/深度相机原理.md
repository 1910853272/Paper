# 深度相机原理

深度相机是一种能够获取场景中物体的距离信息（即深度信息）的相机，与传统的2D相机不同，深度相机不仅能拍摄到物体的二维图像，还能提供每个像素点的三维坐标。深度相机的原理主要有三种：飞行时间法（TOF）、结构光法和双目立体视觉法。

# 1.飞行时间法（TOF）

飞行时间法通过发射特定频率的光脉冲（通常是不可见光）到目标物体上，然后计算光脉冲从相机发射到被物体反射回来的时间。通过这个时间，TOF相机就能计算出物体与相机之间的距离。

![img](https://raw.githubusercontent.com/1910853272/image/master/img/202503261623184.png)

优点：

- 可以提供全场景的深度信息，尤其适用于远距离测量。
- 精度相对较高，适合大范围的测量。

缺点：

- 发射端需要产生高频高强度脉冲，对物理器件性能要求很高
- 对时间测量的精度要求较高，且容易受到环境散射光的干扰。

# 2.结构光法

结构光法通过一个光源将事先设计好的光图案（例如条纹或光斑）编码并投射到物体表面，另一个相机用于捕捉物体表面的图案。图案会因为物体表面的几何形状而发生变形。通过对比原始投射图案和成像图案的差异，推算出物体表面的三维信息得到深度图。

![v2-6be140394cd2c43999108cd700f6564d_1440w](https://raw.githubusercontent.com/1910853272/image/master/img/202503261632462.png)

**优点**：

- 适用于纹理不足或光照不足的场景。
- 精度较高，适用于短到中距离的测量。

**缺点**：

- 受强光影响较大，尤其在室外环境中，太阳光会干扰光图案的投射和接收。

# 3.双目立体视觉法

双目立体视觉法模拟人类眼睛的原理，利用两个相机从不同角度拍摄同一场景。寻找图像中相同物体的像素点并计算其视差。根据已知的两个相机之间的距离和焦距，通过三角测量公式计算出物体的深度信息。

![iShot_2025-03-26_17.00.45](https://raw.githubusercontent.com/1910853272/image/master/img/202503261701809.png)

**优点**：

- 不需要主动光源，完全依赖环境光，适用于室内外环境。
- 成本相对较低，只需要普通的RGB相机。

**缺点**：

- 对环境光照非常敏感，在光照不均或没有纹理的场景中，匹配可能失败。
- 需要较高的计算能力进行图像匹配
- 对相机之间的距离有要求

# 深度相机仿真

## 英特尔RealSense D435深度相机

<img src="../../../../Downloads/depth.png" alt="cover.png" style="zoom:50%;" />

## 相机建模

![](https://raw.githubusercontent.com/1910853272/image/master/img/202503311628236.png)

## 仿真场景

RealSense D435深度相机在Gazebo仿真场景中生成深度图像，将传感器数据传递给ROS处理

![depth-d435-2.png](https://raw.githubusercontent.com/1910853272/image/master/img/202503311630019.png)

# 事件相机



# 光流法

光流是空间运动物体在成像平面上像素运动的瞬时速度。三维空间中运动场反映了物体的运动状态，光流场是三维空间中运动物体在成像平面上的投影所反映出的像素点的运动过程。

![iShot_2025-04-01_15.26.21](https://raw.githubusercontent.com/1910853272/image/master/img/202504011529517.png)

光流法基于如下假设：

1.场景的像素强度在相邻帧之间基本不变。

![iShot_2025-04-01_15.29.51](https://raw.githubusercontent.com/1910853272/image/master/img/202504011530346.png)

2.相邻像素具有相似的运动。对上述公式泰勒级数展开，由运动较小假设省略高阶项。

![iShot_2025-04-01_15.30.37](https://raw.githubusercontent.com/1910853272/image/master/img/202504011531761.png)

![iShot_2025-04-01_15.40.11](https://raw.githubusercontent.com/1910853272/image/master/img/202504011540090.png)

I是梯度变化，u是瞬时速度。

## Lucas Kanade算法

增加约束条件：**假定对于每个像素, 运动场与光流和其邻域中的像素均相同。**

某像素的某领域为W，大小为n×n的矩形，则对W中的每个像素(k,l)∈W，光流都相同。

![iShot_2025-04-01_15.55.29](https://raw.githubusercontent.com/1910853272/image/master/img/202504011556295.png)

```python
cv2.calcOpticalFlowPyrLK()
```

**光流估计值本质上就是n^2个约束方程Au=B的最小二乘解**

## 光流金字塔

运动范围很大时降低图片的分辨率，在低分辨率图像上求解光流。

相邻两帧图像为*I* 和*J*，包含图像信息较多的原始图像放在金字塔的第 0 层，分辨率最低的图像放在金字塔的最顶层。

![iShot_2025-04-01_16.03.03](https://raw.githubusercontent.com/1910853272/image/master/img/202504011603882.png)

在窗口为的区域内存在光流d使得光流残差ε有最小值

![](https://raw.githubusercontent.com/1910853272/image/master/img/202504011608828.png)

g是第$l$层光流估计值

![iShot_2025-04-01_16.08.12](https://raw.githubusercontent.com/1910853272/image/master/img/202504011611937.png)

## 深度学习算法 FlowNet

输入为待估计光流的两张图像，输出即为图像每个像素点的光流。

### 收缩部分网络结构

![1](https://raw.githubusercontent.com/1910853272/image/master/img/202504011630933.png)

卷积网络提取光流信息，池化使图片的分辨率降低

### 放大部分网络结构

![3](https://raw.githubusercontent.com/1910853272/image/master/img/202504011632182.png)

把光流恢复到高像素

### 损失函数

损失函数loss定义为预测的光流和每个像素groundtruth之间的欧式距离，称为EPE(End-Point-Error)。