# 探索具身多模态大模型：发展、数据集与未来方向

本文综述了具身多模态大模型（EMLMs）的发展，包括大型语言模型（LLMs）、大型视觉模型（LVMs）以及其他相关模型。讨论了在具身感知、导航、交互和仿真方面的发展，还分析了用于训练和评估这些模型的数据集。

EMLMs将视觉、语言和音频等多种感官模态输入，生成影响物理世界的输出。

# 关于具身感知、导航和交互领域研究进展的时间线

![iShot_2025-03-31_17.40.38](https://raw.githubusercontent.com/1910853272/image/master/img/202503311740138.png)

不同颜色代表感知、导航、交互、仿真。

具身感知需要视觉感知和推理能力，能够理解场景中的三维关系，并根据视觉信息预测和执行复杂任务



# 具身AI代理的实例

具身代理是具有物理或虚拟身体的自主实体，能够感知、行动并与环境互动，包括机器人、自动驾驶汽车、虚拟代理。

机器人有多种形式，包括固定基座机器人、轮式机器人、四足机器人、人形机器人、软体机器人。

自动驾驶汽车感知环境，做出实时决策，并与驾驶员及周围环境互动。

虚拟代理包括游戏、社会实验、虚拟偶像等。

![iShot_2025-03-31_17.46.03](https://raw.githubusercontent.com/1910853272/image/master/img/202503311746575.png)

## 大型语言模型（LLMs）

Google提出的BERT基于Transformer架构，采用了掩码语言模型进行预训练

OpenAI开发的生成式预训练Transformer（GPT）采用自回归训练生成文本序列；GPT-2进一步扩大了模型规模并提高了性能。

Google提出的XLNet模型将自回归和自编码方法结合；T5统一了所有NLP任务为“文本到文本”框架，使模型能够在不同任务间进行迁移学习。

OpenAI开发的GPT-3极大地提升了在文本生成、问答和翻译等任务中的表现；GPT-3.5及后续版本ChatGPT支持与用户的自然流畅互动，并涉及广泛的知识领域；GPT-4进一步提升了推理能力，并支持多模态输入。

DeepSeek-V3采用动态稀疏激活架构，并引入了一种混合路由机制，结合了任务特定专家和动态参数分配，实现了跨模态融合任务中的更高计算效率。

## 大型视觉模型（LVMs）

**ResNet**是一种深度卷积神经网络，引入残差连接解决训练非常深的神经网络时可能遇到的梯度消失和梯度爆炸问题。

**Vision Transformer（ViT）**不依赖于传统的卷积操作，而是将图像划分为固定大小的块，并利用自注意力机制来捕捉图像中的全局依赖关系。

**Swin Transformer**具有改进的局部自注意力机制，能够有效捕捉局部信息，同时保持全局上下文。

**Meta推出的Segment Anything Model（SAM）**能够处理多种分割任务，包括语义分割、实例分割和物体分割

**DINOv2**是Facebook AI团队提出的自监督学习模型，建立在ViT架构上，具有更强的图像表示学习能力。

## 大型视觉-语言模型（LVLMs）

**CLIP：**OpenAI推出的一个多模态模型，旨在通过对比学习技术将图像和文本嵌入到共享的向量空间中，能够执行图像分类和图像-文本检索等任务

**DALL·E：**一种图像生成模型，通过文本提示生成图像，能够创造高度现实和富有创意的图像

**BLIP：**通过双向自监督学习方法整合视觉和语言信息，特别擅长于视觉问题解答（VQA）和图像描述任务。

**Flamingo：**DeepMind的一种新型视觉-语言模型，能够处理多模态数据（图像和文本），并执行跨模态推理，特别在少样本学习方面表现出色。

**Visual BERT**：这是Facebook AI Research开发的BERT模型的一个变种，集成了视觉信息

**ALIGN**：这是Google Research推出的大规模图像-文本对齐模型

**GIT**：这是Microsoft Research开发的预训练模型，旨在处理图像-文本生成和理解任务

**MDETR**：这是Facebook AI Research开发的一种基于Transformer的视觉-语言模型。

**PaLM-E**：这是Google Research提出的强大的多模态预训练模型，结合了三种模态：图像、文本和动作（如机器人操作）

**CoCa**：这是Meta推出的一种新型模型，结合了对比学习和生成学习

## 视觉-音频模型

**SoundSpaces**：结合了视觉和音频，用于执行AudioGoal（目标由声音指示）和AudioPointGoal（音频提供方向指引）等任务。

## 视觉-触觉模型

集成了视觉和触觉数据，用于估计物体在手中操作时的6D姿态（位置和姿态）

通过视觉和触觉反馈来迭代地调整抓取方法，从而在有触觉信息的情况下提高了抓取的表现。

# 具身代理的完整任务栈

![iShot_2025-03-31_19.18.52](https://raw.githubusercontent.com/1910853272/image/master/img/202503311918828.png)

## 1.具身感知

### 基于GPT的大型模型

**Octopus**：使用GPT-4V动态生成观察到的图像的描述和分析，包括场景中可以互动的物体及其相对位置。这些描述作为输入提供给语言模型，生成下一步行动的决策。

**CoPa**：使用GPT-4V直接识别并突出图像中的抓取区域和潜在的抓取姿势，实现细粒度的物体理解。然后，GraspNet作为抓取姿势检测器，选择信心度最高的抓取姿势进行执行。

**Voxposer**：通过多模态大模型提取物体的空间几何信息，并生成其3D坐标。这些物体坐标用于填充代码中的参数，生成一系列3D功能图和约束图。

**ReKep**：比Voxposer方法更具适应性，能够更好地应对环境变化。它使用DINOv2提取RGB图像的特征，经过双线性插值采样后与Segment Anything Model结合提取场景中的标记，然后将图像和任务指令输入GPT-4o模型，生成所需的阶段和子目标及路径约束。

**RobotGPT**：使用GPT直接生成可执行代码，提出了由背景信息、物体细节、环境上下文、任务规格和示例组成的五部分提示方法。通过详细描述环境、物体和任务，引导ChatGPT生成精确的输出，并通过生成的代码控制机器人执行指定任务。

### 基于非GPT的大型模型方法

**PaLM-E**：直接集成来自具身代理传感器模态的连续输入，使得语言模型能够进行更为接地气的推理，从而支持在现实世界中的连续决策。输入如图像和状态估计被嵌入到与语言标记相同的潜在空间中，通过Transformer模型的自注意力层处理。

**ViT和RT-1**：RT-1使用EfficientNet-B3作为感知模块，将图像数据编码成隐式表示以供后续任务执行。经过训练，该感知模块能够以增强任务执行效果的方式编码信息。

**RT-2**：作为RT-1的升级版，RT-2直接训练视觉-语言模型，输出低级机器人动作，并将这些动作表示为文本标记。通过与互联网规模的视觉-语言任务一起训练，提升了其性能和多功能性。

**RT-H**：结合RT-1和RT-2的优点，并通过使用PaLI-X 55B架构进一步改进，采用ViT模型对图像进行标记，然后使用编码器-解码器Transformer将图像和自然语言标记流转化为动作标签。

**RoboFlamingo**：基于开放源代码视觉-语言模型OpenFlamingo，经过少量模仿学习的微调，专门用于下游的操控任务。它的视觉编码器包括ViT和Perceiver Resampler，灵活的设计使其能够在性能较低的平台上运行。

**Openvla**：一个拥有70亿参数的开源视觉-语言-动作（VLA）模型，经过对970,000个真实世界机器人示范的多样化数据集的训练。它增强了Llama 2语言模型，融合了来自DINOv2和SigLIP的预训练视觉编码器，以提供细粒度的空间推理和丰富的视觉语义理解。

![iShot_2025-04-02_14.10.05](https://raw.githubusercontent.com/1910853272/image/master/img/202504021410875.png)

## 2.具身导航

### 通用大模型

**LM-Nav**：利用GPT-3模型进行自然语言指令解析，通过提取文本中的地标并与场景图像结合，做出导航决策。

**L3MVN**：使用RoBERTa-large模型，通过两种创新的方法进行导航任务。其“零-shot方法”生成语义描述句子并通过LVLMs评估导航目标的存在概率；而“前馈方法”则将查询句子编码为总结嵌入，输入神经网络进行目标概率预测。

**VLMaps**：在L3MVN的基础上，采用类似的“零-shot”方法并引入GPT-4v，帮助模型推测目标附近最有可能出现的已知物体。

**NavGPT**：完全基于LLMs的系统，用于理解和执行自然语言导航指令。它能够处理多模态输入，包括文本描述、视觉观察、导航历史和未来的探索方向。

**SG-Nav**：构建了一个层次化的3D场景图，利用LLMs进行层次化推理，推测目标物体的位置、预测距离并评估目标的可信度，从而增强导航的准确性和鲁棒性。

**CLIP**：通过图像-文本对的预训练，建立跨模态语义理解，帮助机器人在导航任务中将环境图像与文本指令或地标建立联系。CLIP也可以与强化学习结合，提高机器人对环境的理解和决策能力。

**BLIP和BLIP-2**：这两个模型在视觉-语言任务中具有更强的能力，特别是BLIP-2能够直接从图像生成文本描述，为模型提供基于输入图像内容的文本描述，而无需额外的训练数据。

**NavGPT-2**：基于InstructBLIP和EVA-CLIP的视觉编码器，增强了LLMs的推理能力，并在视觉语言导航（VLN）任务中表现出色。

**vlfm**：利用BLIP-2计算当前RGB观察帧与目标文本提示中每个对象的余弦相似度，生成2D值图，然后与生成的前沿值图结合，确定最佳航点。

### 专用大模型

**NavCoT**：结合现有的视觉语言导航（VLN）数据集和先进模型（如LLMs和CLIP），训练LLaMA模型，以从多视角图像中提取物体文本，增强其在未来想象任务中的表现，提升导航决策过程。

**NaviLLM**：使用ViT提取每个位置的六个视角图像特征，形成场景编码，然后通过Transformer编码器捕捉不同视角之间的相互依赖性，再通过LLM生成任务指令、观察结果和历史信息。

**Trans-EQA**：利用Transformer的全局建模优势，在导航模块中替代传统CNN的局部特征提取瓶颈，有效地将分散的视觉特征与语言语义关联。

**GOAT**：将视觉、指令和历史信息分解为中介、可观察的混杂因子和不可观察的混杂因子。提出的BACL和FACL因果学习模块能够处理这些信息，提供高通用性，适应复杂和未见过的环境，能够在实际应用中减少数据集偏差的影响。

**Rui Liu等人提出的模型**：使用体素实现更全面的3D表示，结合ViT和交叉视角注意力（CVA）方法，提升了2D到3D之间的粗略采样，最终实现了细粒度的3D表示。

**GNM、ViNT和NoMaD**：由伯克利人工智能研究团队推出的一组通用模型，能够控制多种不同的机器人平台，适应不同的导航任务。GNM通过60小时的异质数据集进行训练，能够跨多个机器人平台广泛推广；ViNT基于Transformer架构，能够通过预训练学习通用导航能力，并在不同任务中实现快速适应；NoMaD结合Transformer和扩散模型，能够在任务导向导航和无任务探索之间统一处理，提升了机器人在未知环境中的导航性能。

![iShot_2025-04-02_14.19.56](https://raw.githubusercontent.com/1910853272/image/master/img/202504021420390.png)

## 3.具身交互

### 基于语言的短期行动策略

**R3M**：提出了一种视觉编码器，通过使用大量Ego4D数据集进行训练，学习稀疏和紧凑的表示，从而提高机器人操作的成功率，比CLIP和MoCo等视觉编码器的成功率提高了10%。

**Vi-PRoM**：进一步改进了R3M，探索了视觉编码器的预训练策略，特别关注数据集、模型架构和训练方法，实验结果显示其表现优于R3M。

**RT-1**：通过大量开放空间数据进行训练，提升机器人在未知场景中的多任务学习能力和泛化能力，探索了数据规模、模型参数大小和数据多样性等三个方面的影响。Google DeepMind基于RT-1提出了预训练的视觉语言模型MOO，能够实现新环境和新物体的零-shot学习。

**Q-transformer**：基于RT-1的方法，使用Transformer来训练和学习每个行动的Q值，优于传统的离散强化学习和模仿学习方法。

**RT-2**：在RT-1的基础上，结合了大型语言模型和新的视觉编码器，形成了一种新的视觉-语言-行动模型，显著提高了物体的泛化能力。

**Octo**：提出了一种政策转移的微调策略，用于不同观察和机器人动作之间的转换，通过九种不同平台的机器人测试，证明了该方法的有效性。

**RoboFlamingo**：基于开源视觉语言模型OpenFlamingo进行微调，构建了一种成本效益高且易于使用的机器人行动框架。

**Vima**：基于Transformer的机器人行动策略，通过将目标的视觉裁剪图像引入提示，实验结果显示在零-shot泛化任务中，任务成功率是其他方法的2.9倍。

**RT-H**：首先预测行动语言，然后根据行动语言和视觉信息预测机器人行动，实验表明，这种语言行动层次结构比传统方法更强大和灵活。

**Openvla**：基于开源视觉-语言模型prismatic-vlm，结合预训练的DINOv2和SigLIP视觉编码器及Llama 2语言模型，显著提高了任务成功率。

**Hiveformer**：结合自然语言指令、多视角场景观察和历史记录来输出机器人行动，实验表明，该方法在语言条件指令上表现优异，具有出色的泛化性能。

**GR-1**：一个简单的GPT式端到端模型，输入语言指令、一系列观察到的图像和机器人状态，预测机器人行动及未来的图像。

**Voxposer**：利用大型语言模型的代码写作功能与视觉语言模型互动，形成3D值图，并在基于模型的规划框架中使用该值图合成机器人轨迹。

### 基于语言的长期行动策略

**SayCan**：通过预训练技能将大型语言模型与现实世界连接，结合低级技能和大型语言模型帮助机器人完成复杂任务，能够执行长期的自然语言指令和复杂任务。

**Zero-Shot Planners**：利用大型语言模型学习的世界知识，通过描述任务的提示实现任务规划和分解，无需额外的训练。

**Text2Motion**：提出了一种基于语言的规划框架，能够解决长期序列操作任务，生成任务和运动计划，并验证计划是否完成。

**EmbodiedGPT**：结合视觉和语言信息提取任务相关特征，实现高效准确的任务规划。

**Palm-e**：基于预训练的大型模型进行端到端训练，通过输入语言和图像信息实现视觉语言模型的泛化。

**TPVQA**：利用视觉语言模型检测任务是否成功执行，并将任务分解为子任务序列。

**TaPA**：在具身任务中提出任务规划，将LLMs与视觉感知模型对齐，根据场景中感知的物体生成可执行的任务序列。

**ViLaIn**：提出了一种视觉语言解释器，利用LLMs和视觉语言模型生成问题描述，通过符号规划器的反馈优化生成的问题描述。

**PG-InstructBLIP**：结合基于物理世界的LVLM和大型语言模型，构建机器人规划的互动框架，在需要推理物体概念的任务中表现优越。

**Model ision**：使用增强版视觉语言模型GPT-4V，输入操作演示视频和自然语言指令，编码成符号任务计划，展示出高效的学习能力。

**ScreenAgent**：结合任务规划和运动规划，提出了Autotamp方法，将自然语言任务描述转换为中间任务表示，结合传统的任务和运动规划算法解决任务和运动规划问题。

**Mutex**：提出了一种多模态任务规格的统一学习方法，结合跨模态任务学习和Transformer架构，能够执行序列文本任务指令和多任务文本指令。

**LEO**：开发了一个具身多模态通用代理，在3D世界中表现出色，能够进行感知、推理、规划和执行任务，证明了3D视觉语言对齐和3D视觉语言行动指令调整的有效性。

**Sayplan**：提出了一种可扩展的方法，通过LLMs和3D场景图表示构建机器人任务规划，实验表明该方法能够为机器人执行大规模、长期的任务规划打下基础。

![iShot_2025-04-02_14.28.27](https://raw.githubusercontent.com/1910853272/image/master/img/202504021428611.png)

![iShot_2025-04-02_14.28.38](https://raw.githubusercontent.com/1910853272/image/master/img/202504021429431.png)

![iShot_2025-04-02_14.28.46](https://raw.githubusercontent.com/1910853272/image/master/img/202504021430013.png)

## 4.具身模拟

### 基于基础模拟的通用模拟器

**NVIDIA Omniverse™ Isaac Sim**支持创建高度准确、物理真实的模拟和合成数据集，提供如高级物理模拟和多传感器RTX渲染等功能。支持ROS2，Isaac Sim有助于机器人设计、调试、训练和部署，加速自动系统的发展。

### 基于现实世界场景的模拟器

**TRUMANS数据集** 和 **自回归运动扩散模型**：提出了一种用于生成人体与场景交互（HSI）序列的自回归运动扩散模型。

**WonderWorld框架**：解决了现有方法在3D场景生成时的离线生成速度慢和场景几何畸变的问题，提供低延迟的用户交互。

**GenZI**：一种开创性的零-shot方法，旨在根据文本描述生成3D人体与场景的交互（HSI）。

**iGibson 2.0和Habitat-Sim**：iGibson 2.0关注生成具有互动性的3D环境，模拟各种家庭任务，支持多种物体状态（如温度、湿度、清洁度）和逻辑谓词，兼容商业虚拟现实（VR）系统，使用户能够在虚拟场景中进行交互。

**Genesis**：这是最新的技术，采用新开发的通用物理引擎，整合了多种物理求解器及其交互到一个统一框架中。基于此核心引擎，提出了一种生成代理框架，用于机器人和其他领域的自动化数据生成。

![iShot_2025-04-02_14.36.35](https://raw.githubusercontent.com/1910853272/image/master/img/202504021436073.png)

# 数据集

## 1.具身数据集收集方法

**现实世界数据集收集**：这些数据集使用各种传感器（如RGB相机、深度相机、惯性测量单元（IMU）、激光雷达（LiDAR）、压力传感器、声音传感器等）收集数据。

**模拟器数据集收集**：使用模拟器（如Unity和Gazebo）可以快速生成大量的多模态数据（例如图像、深度图、传感器数据等），并能控制环境和任务变量，有助于模型训练。

## 2.具身感知与交互数据集

**Open X-Embodiment数据集**：由Google团队与20多个组织和研究机构合作发布，提供了一个大规模的多模态资源。它包含来自22种类型的机器人数据，涵盖RGB图像、端点运动轨迹和语言命令，总共有1百万个场景、500多个技能和150,000个任务。该数据集包含60个子数据集，其中一些可以参见表5。

**RH20T数据集**：由Hao-Shu Fang等人提出，包含超过110,000个机器人操作序列，涵盖视觉、力学、音频、运动轨迹、演示视频和自然语言指令，是训练具身智能模型的重要资源。

**ManiWAV数据集**：该数据集使用“耳入手”数据收集设备捕捉人类演示数据，提供同步的音频和视觉反馈，是学习机器人操控策略的丰富数据源。

**ARIO数据集**：由彭程实验室开发，包含超过300万样本，涵盖图像、语言命令、触觉反馈和语音，数据既来自现实场景也来自模拟平台，支持多模态学习。

**ManiSkill和ManiSkill2数据集**：来自加州大学圣地亚哥分校，包含36,000个成功的操控轨迹和150万点云与RGB-D帧，主要通过模拟收集数据。

**DROID数据集**：包含76,000个演示轨迹和多模态数据，针对机器人操控和交互任务提供了数据支持。

**TACO-RL数据集**：用于训练层次化策略解决长期机器人控制任务，通过在模拟和现实环境中遥控机器人来收集数据。

**FurnitureBench数据集**：专注于测试与家具组装相关的复杂长期任务，强调精确抓取、路径规划和插入技能。

**Dexterous Hand数据集**：由苏黎世联邦理工学院（ETH Zurich）推出，包含2.1百万帧视频、3D手部和物体网格、动态接触信息等，提供了关于灵巧手操作的宝贵数据。

## 3.具身感知与交互数据集

提供了复杂路径和指令的标注，涵盖了现实世界数据、室内外多种场景、支持大规模模型训练、以及3D场景重建、相对深度估计、物体标签和定位信息等中间产品。

**HM3D数据集**：这是最大规模的建筑级3D场景重建数据集，包含来自世界各地的多样化真实空间。HM3D提供了1,000个几乎完整的建筑重建，使用Matterport Pro2深度传感器捕捉每个室内空间的可居住和可导航区域。该数据集与FAIR的Habitat模拟器无缝集成，支持训练和评估智能代理。

**Gibson环境**：Gibson环境用于训练和测试感知智能代理。它基于虚拟化的真实空间，当前包括来自572个完整建筑的1400多个楼层空间，反映了现实世界的语义复杂性，且支持模型无须领域适配直接部署到现实世界。

**Matterport3D数据集**：这是一个大规模的RGB-D数据集，包含来自90个建筑规模场景的10,800个全景视图和194,400个RGB-D图像，提供表面重建、相机姿态以及2D和3D语义分割的注释。

**R2R数据集**：基于Matterport3D环境，包含21,567条开放词汇、众包的导航指令，目标是通过自然语言指令让机器人在之前未见过的建筑中导航到目标地点。

**REVERIE数据集**：基于Matterport3D模拟器，提供详细的室内导航环境，新增了“远程具身视觉指代表达”任务，要求代理在基于自然语言指令的情况下导航并识别远程物体。

**ScanQA数据集**：设计用于3D场景理解中的问答任务，要求模型接收来自3D场景的视觉信息并回答关于该场景的文本问题。该数据集包含来自ScanNet数据集的41,000多个问题-答案对，旨在推动3D空间理解研究。

**LLaVA数据集**：这是一个专门用于多模态任务指令调整的数据集，包含158K独特的语言-图像指令跟随样本，支持基于视觉和文本信息理解并执行指令，能够增强模型在复杂环境中的导航和执行能力。

**SoundSpaces模拟器**：基于Matterport3D和Replica，设计用于辅助音频导航任务，包含现实音效渲染和85个场景，模拟了不同空间和材质属性。

![iShot_2025-04-02_14.41.34](https://raw.githubusercontent.com/1910853272/image/master/img/202504021441903.png)

![iShot_2025-04-02_14.41.55](https://raw.githubusercontent.com/1910853272/image/master/img/202504021443438.png)

![iShot_2025-04-02_14.42.06](https://raw.githubusercontent.com/1910853272/image/master/img/202504021443519.png)

![iShot_2025-04-02_14.42.16](https://raw.githubusercontent.com/1910853272/image/master/img/202504021443282.png)

![iShot_2025-04-02_14.42.24](https://raw.githubusercontent.com/1910853272/image/master/img/202504021443992.png)

