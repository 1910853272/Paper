# Exploring Embodied Multimodal Large Models-Development, Datasets, and Future Directions

## 1.具身数据集收集方法

**现实世界数据集收集**：这些数据集使用各种传感器（如RGB相机、深度相机、惯性测量单元（IMU）、激光雷达（LiDAR）、压力传感器、声音传感器等）收集数据。

**模拟器数据集收集**：使用模拟器（如Unity和Gazebo）可以快速生成大量的多模态数据（例如图像、深度图、传感器数据等），并能控制环境和任务变量，有助于模型训练。

## 2.具身感知与交互数据集

**Open X-Embodiment数据集**：由Google团队与20多个组织和研究机构合作发布，提供了一个大规模的多模态资源。它包含来自22种类型的机器人数据，涵盖RGB图像、端点运动轨迹和语言命令，总共有1百万个场景、500多个技能和150,000个任务。

**RH20T数据集**：由Hao-Shu Fang等人提出，包含超过110,000个机器人操作序列，涵盖视觉、力学、音频、运动轨迹、演示视频和自然语言指令，是训练具身智能模型的重要资源。

**ManiWAV数据集**：该数据集使用“耳入手”数据收集设备捕捉人类演示数据，提供同步的音频和视觉反馈，是学习机器人操控策略的丰富数据源。

**ARIO数据集**：由彭程实验室开发，包含超过300万样本，涵盖图像、语言命令、触觉反馈和语音，数据既来自现实场景也来自模拟平台，支持多模态学习。

**ManiSkill和ManiSkill2数据集**：来自加州大学圣地亚哥分校，包含36,000个成功的操控轨迹和150万点云与RGB-D帧，主要通过模拟收集数据。

**DROID数据集**：包含76,000个演示轨迹和多模态数据，针对机器人操控和交互任务提供了数据支持。

**TACO-RL数据集**：用于训练层次化策略解决长期机器人控制任务，通过在模拟和现实环境中遥控机器人来收集数据。

**FurnitureBench数据集**：专注于测试与家具组装相关的复杂长期任务，强调精确抓取、路径规划和插入技能。

**Dexterous Hand数据集**：由苏黎世联邦理工学院（ETH Zurich）推出，包含2.1百万帧视频、3D手部和物体网格、动态接触信息等，提供了关于灵巧手操作的宝贵数据。

## 3.具身感知与交互数据集

提供了复杂路径和指令的标注，涵盖了现实世界数据、室内外多种场景、支持大规模模型训练、以及3D场景重建、相对深度估计、物体标签和定位信息等中间产品。

**HM3D数据集**：这是最大规模的建筑级3D场景重建数据集，包含来自世界各地的多样化真实空间。HM3D提供了1,000个几乎完整的建筑重建，使用Matterport Pro2深度传感器捕捉每个室内空间的可居住和可导航区域。该数据集与FAIR的Habitat模拟器无缝集成，支持训练和评估智能代理。

**Gibson环境**：Gibson环境用于训练和测试感知智能代理。它基于虚拟化的真实空间，当前包括来自572个完整建筑的1400多个楼层空间，反映了现实世界的语义复杂性，且支持模型无须领域适配直接部署到现实世界。

**Matterport3D数据集**：这是一个大规模的RGB-D数据集，包含来自90个建筑规模场景的10,800个全景视图和194,400个RGB-D图像，提供表面重建、相机姿态以及2D和3D语义分割的注释。

**R2R数据集**：基于Matterport3D环境，包含21,567条开放词汇、众包的导航指令，目标是通过自然语言指令让机器人在之前未见过的建筑中导航到目标地点。

**REVERIE数据集**：基于Matterport3D模拟器，提供详细的室内导航环境，新增了“远程具身视觉指代表达”任务，要求代理在基于自然语言指令的情况下导航并识别远程物体。

**ScanQA数据集**：设计用于3D场景理解中的问答任务，要求模型接收来自3D场景的视觉信息并回答关于该场景的文本问题。该数据集包含来自ScanNet数据集的41,000多个问题-答案对，旨在推动3D空间理解研究。

**LLaVA数据集**：这是一个专门用于多模态任务指令调整的数据集，包含158K独特的语言-图像指令跟随样本，支持基于视觉和文本信息理解并执行指令，能够增强模型在复杂环境中的导航和执行能力。

**SoundSpaces模拟器**：基于Matterport3D和Replica，设计用于辅助音频导航任务，包含现实音效渲染和85个场景，模拟了不同空间和材质属性。

![iShot_2025-04-02_14.41.34](https://raw.githubusercontent.com/1910853272/image/master/img/202504021441903.png)

![iShot_2025-04-02_14.41.55](https://raw.githubusercontent.com/1910853272/image/master/img/202504021450651.png)

![iShot_2025-04-02_14.42.06](https://raw.githubusercontent.com/1910853272/image/master/img/202504021443519.png)

![iShot_2025-04-02_14.42.16](https://raw.githubusercontent.com/1910853272/image/master/img/202504021443282.png)

![iShot_2025-04-02_14.42.24](https://raw.githubusercontent.com/1910853272/image/master/img/202504021450853.png)

# Open X-Embodiment - Robotic Learning Datasets and RT-X Models

## Open X-Embodiment 数据集

整合了来自 34 个机器人实验室 的 60 个现有数据集。采用 RLDS 数据格式，统一存储为 TFRecord 文件，支持：不同机器人的动作空间（如关节控制、末端执行器控制）、多种输入模态（RGB 相机、深度相机、点云等）、高效的并行数据加载（兼容主流深度学习框架）。

![iShot_2025-04-02_15.31.11](https://raw.githubusercontent.com/1910853272/image/master/img/202504021948458.png)

a.**机器人类型分布**

b.**场景多样性**

c.**轨迹数量**

d.**常见数据集技能**

e.**常见数据集物体**

##  RT-X 模型设计

![iShot_2025-04-02_15.51.35](https://raw.githubusercontent.com/1910853272/image/master/img/202504021553366.png)

### 1.数据格式统一

**观测输入**：

- 模型接收历史图像序列和语言指令。
- 从每个数据集中选择一个标准视角的RGB图像，统一分辨率（如调整至固定尺寸）。

**动作输出**：

- 预测7维末端执行器动作向量（x, y, z, roll, pitch, yaw, 夹爪开合度或其变化率）。
- 对原始动作进行归一化和离散化，便于模型输出。

**灵活性保留**：

- 相机位姿和控制坐标系未强制对齐，允许不同机器人对相同动作向量产生不同运动

### 2.RT-1-X：专用机器人控制优化

**输入**：15帧历史图像（通过ImageNet预训练的EfficientNet 提取特征）、语言指令（编码为USE嵌入向量）

**特征融合**：使用FiLM层交织视觉与语言特征，生成81个多模态token。

**动作预测**：通过纯解码器Transformer输出离散化动作（8维，含1维终止标志+7维运动控制）。

### 3.RT-2-X：基于视觉-语言模型（VLM）

基于大规模视觉-语言模型（VLM）微调，将动作预测转化为文本token生成任务

**视觉部分**：ViT（Vision Transformer） 处理图像。

**语言部分**：UL2 编码文本指令。

**预训练数据：**WebLI 

# Towards Generalist Robot Learning from Internet Video - A Survey

## LfV的视频数据集

### 1.关键特性

规模与多样性：规模通常通过视频总时长衡量，多样性则指任务、场景、动作类型的广泛分布

内容相关性：视频内容应与泛化机器人任务场景相关

语言标注：包括高级语言描述（如抽象场景描述）和细粒度标注（如动作细节、空间信息、物体关系）。

其他：场景连续性、高分辨率、长时序视频、多模态数据

### 2.构建方法

网络爬取：YouTube等平台爬取，（生成查询提示词、视频剪辑与元数据筛选、重新标注视频）

自录视频：人类、遥控机器人或实体机器人执行相关任务

语言标注：1.采用语音记录代替文本输入，后续通过人工补充细节以提升描述性。2.使用现成视觉模型（图像生成文本、物体检测）或利用大语言模型（LLM）对多源内容进行摘要、过滤和整合。

### 3.现有数据集

![iShot_2025-04-14_15.52.00](https://raw.githubusercontent.com/1910853272/image/master/img/202504141552325.png)

1.大规模网络抓取数据集：（InternVid、HowTo100M、WebVid-10M）视频时长可达数十万小时；来源广泛、内容多样；采用自动语言标注方式；用于训练视频基础模型。

2.人工采集视频数据集：（Ego4D、RoboVQA、Epic-Kitchens）视频总量较少，但内容与机器人任务高度相关；多为人工标注；常被用于视频任务微调或特定任务评估。

3.所有数据集均提供文本标注；部分数据集还包括音频、三维网格、凝视轨迹、立体视频、手部标注等多模态数据。



# All Robots in One - A New Standard and Unified Dataset for Versatile, General-Purpose Embodied Agents

## ARIO标准

### 1.层次化数据结构

| **Collection（数据集合集）** | 包含多个 series，代表一个完整数据来源                        |
| ---------------------------- | ------------------------------------------------------------ |
| **Series（系列）**           | 对应特定的场景与机器人类型，如“厨房场景 + UR5”               |
| **Task（任务）**             | 每个 series 中定义多个任务，如“拿起一个苹果”，用自然语言说明 |
| **Episode（回合）**          | 每个任务下具体的一次操作执行，包括所有观察与控制数据，按统一时间戳同步 |

### 2.数据采集协议

每次数据采集需包含：

通用模态：如**任务文本说明**、**图像**任务

特定模态：如**末端执行器状态**（抓取姿态、力度等）、**导航数据**等

### 3.元数据与文档说明

`information.yaml`：每个 series 提供的说明文档，包含场景信息、机器人类型、传感器种类等；

`description.yaml`：每个任务的详细说明文档，包括任务目标、所需技能等内容。

###  4.标准化与数据完整性

统一的数据格式（跨机器人平台、控制方式）严格的数据采集规范（传感器同步、格式规范）

## ARIO数据集构建方法

![iShot_2025-04-14_16.31.55](https://raw.githubusercontent.com/1910853272/image/master/img/202504141632669.png)

### 1.真实机器人数据采集

Cobot Magic双臂+移动底盘，支持力控制、粗触觉反馈，使用Orbbec RGB-D摄像头、200Hz关节数据记录

Cloud Ginger XR-1高自由度轮式人形机器人。

### 2.模拟平台生成数据

Habitat + HM3D：进行目标导航任务，收集Agent的RGB-D观测与位置信息

MuJoCo：基于LLM引导的机器人操作场景（如抓取、放置、开抽屉）

SeaWave（UE5）：语言引导下的复杂交互任务，含抽象指令与决策过程

### 3.转换已有开源数据集

Open X-Embodiment（72子集，2.4M条数据）：标准不统一，通过自研工具转换为ARIO格式

RH20T：真实遥操作任务，含多视角RGBD+触觉+动作信息

ManiWAV：唯一包含音频模态的机器人数据集，任务如擦白板、翻贝果等

# RoboMIND - Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation

## 数据收集和处理

### 1.数据采集与存储

通过遥操作方式采集数据，所有数据最终被打包为 H5 文件，包含视觉信息、机器人状态、末端执行器状态以及遥操作者身体状态。

**单臂机器人（如 Franka、UR5e）**：使用 3D 打印部件和伺服电机来模拟人手动作，控制机械臂，并用深度相机采集 RGB-D 图像及本体状态；

**双臂机器人（AgileX）**：借助双臂遥操作设备（类似 Mobile ALOHA）进行控制，并同步采集数据；

**类人机器人（Tien Kung）**：

- 结构模仿人类，拥有 42 个自由度（DoF）；
- 配备了头部、胸部、腰部和背部的深度摄像头（Orbbec Gemini 335/335L），实现多视角感知；
- 使用 Xsens 动作捕捉服 捕捉人类关节运动，并映射到机器人，实现高精度模仿。

![iShot_2025-04-14_17.02.22](https://raw.githubusercontent.com/1910853272/image/master/img/202504141702607.png)

### 2.数据预处理与分类

**初检**：快速浏览视频，排除明显技术问题（如卡顿、丢帧）；

**细检**：逐帧查看视频，确认是否存在不符合标准的问题；

**记录与筛选**：标注问题时间点及类型，将不合格轨迹归档处理



任务分类基于统一的任务命名方式，每个任务由以下要素定义：

- 所使用的机器人具身形式；
- 所执行的操作技能；
- 涉及的物体；
- 场景中的位置关系与环境约束。

### 3.数据注释

![iShot_2025-04-14_17.01.11](https://raw.githubusercontent.com/1910853272/image/master/img/202504141701877.png)

使用 Gemini 模型将视频自动划分为若干动作片段，并生成初步描述；

人工修正以下内容：

- 明确涉及的关键物体；
- 全面识别与描述关键动作；
- 精准表达操作细节；
- 合理划分动作粒度；
- 保持时间顺序逻辑一致性

## 数据集分析

### 1.多种机器人具身形式

![iShot_2025-04-14_17.05.54](https://raw.githubusercontent.com/1910853272/image/master/img/202504141706899.png)

单臂机器人 Franka Emika Panda 和带夹持器的 UR5e;

带抓手的双臂机器人 AgileX Cobot Magic V2.0;

配备灵巧双手的人形机器人 Tien Kung

### 2.任务时长分布

**Franka 和 UR5e**：平均轨迹步数 < 200，适用于原子操作技能学习；

**Tien Kung 和 AgileX**：轨迹较长（> 500 步），适用于长时序技能组合训练

### 3.任务类型分类

1. **关节类操作（Articulated M.）**：如开关门、开抽屉；
2. **协调类操作（Coordination M.）**：双臂协调；
3. **基础操作（Basic M.）**：抓取、放置、搬运等基本技能；
4. **多物体交互（Obj. Int.）**：如推动一个物体穿越另一个；
5. **精细操作（Precision M.）**：如倒液体、装配电池等；
6. **场景理解（Scene U.）**：如识别方位和语义关系执行任务。

### 4.多样化物体类别

RoboMIND 涵盖了 5 大类使用场景的 96 种物体

![iShot_2025-04-14_17.06.47](https://raw.githubusercontent.com/1910853272/image/master/img/202504141708806.png)